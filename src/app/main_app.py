import streamlit as st
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
from pathlib import Path
from statsmodels.tsa.seasonal import seasonal_decompose # ç”¨äºæ—¶é—´åºåˆ—åˆ†è§£å›¾
from sklearn.model_selection import train_test_split # ç¡®ä¿æ­£ç¡®å¯¼å…¥
import sys


# --- é…ç½® Streamlit é¡µé¢ ---
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, '..', '..')) # è·å–é¡¹ç›®æ ¹ç›®å½• (disbusiness)
sys.path.append(project_root) # å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° sys.path
sys.path.append(os.path.join(project_root, 'src')) # å°† src ç›®å½•ä¹Ÿæ·»åŠ åˆ° sys.pathï¼Œä¾¿äºæ¨¡å—æŸ¥æ‰¾

try:
    from src.data_processing.processor import DataProcessor
    from src.modeling.forecaster import SalesForecaster
    from src.visualization.plotter import Plotter
except ImportError as e:
    st.error(f"å¯¼å…¥æ¨¡å—æ—¶å‡ºé”™: {e}ã€‚è¯·ç¡®ä¿æ‚¨çš„ PYTHONPATH è®¾ç½®æ­£ç¡®æˆ–è°ƒæ•´å¯¼å…¥è·¯å¾„ã€‚")
    st.write("å½“å‰ sys.path:", sys.path)
    st.write("å½“å‰å·¥ä½œç›®å½•:", os.getcwd())
    st.stop()

# --- é…ç½®ä¸å¸¸é‡ ---
RAW_DATA_DIR = os.path.join(project_root, "data", "raw")
PROCESSED_DATA_DIR = os.path.join(project_root, "data", "processed")
MODEL_DIR = os.path.join(project_root, "models")
REPORTS_FIGURES_DIR = os.path.join(project_root, "reports", "figures")
DEFAULT_DATA_FILE = "Online Retail.xlsx" # é»˜è®¤æ•°æ®é›†æ–‡ä»¶å

# ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
for dir_path in [RAW_DATA_DIR, PROCESSED_DATA_DIR, MODEL_DIR, REPORTS_FIGURES_DIR]:
    os.makedirs(dir_path, exist_ok=True)

# --- åº”ç”¨çŠ¶æ€ç®¡ç† (ä½¿ç”¨ st.session_state) ---
def initialize_session_state():
    """åˆå§‹åŒ– Streamlitä¼šè¯çŠ¶æ€å˜é‡ã€‚"""
    if 'raw_df' not in st.session_state:
        st.session_state.raw_df = None
    if 'processed_df' not in st.session_state:
        st.session_state.processed_df = None
    if 'trained_model' not in st.session_state:
        st.session_state.trained_model = None
    if 'plotter' not in st.session_state:
        st.session_state.plotter = Plotter() # åˆå§‹åŒ–ç»˜å›¾å™¨ä¸€æ¬¡
    if 'data_processor' not in st.session_state:
        st.session_state.data_processor = None # æ•°æ®åŠ è½½ååˆå§‹åŒ–
    if 'X_test_data' not in st.session_state:
        st.session_state.X_test_data = None
    if 'y_test_data' not in st.session_state:
        st.session_state.y_test_data = None
    if 'predictions' not in st.session_state:
        st.session_state.predictions = None
    if 'last_trained_model_name' not in st.session_state:
        st.session_state.last_trained_model_name = None
    if 'last_saved_model_path' not in st.session_state:
        st.session_state.last_saved_model_path = None

initialize_session_state()

# --- è¾…åŠ©å‡½æ•° ---
def load_data_from_path(file_path):
    """ä»æŒ‡å®šè·¯å¾„åŠ è½½æ•°æ®å¹¶æ›´æ–°ä¼šè¯çŠ¶æ€ã€‚"""
    try:
        data_processor = DataProcessor(data_path=file_path)
        st.session_state.data_processor = data_processor
        st.session_state.raw_df = data_processor.load_data()
        if st.session_state.raw_df is not None:
            st.success(f"æ•°æ® '{os.path.basename(file_path)}' åŠ è½½æˆåŠŸ!")
            st.session_state.processed_df = None # é‡ç½®å·²å¤„ç†æ•°æ®
            st.session_state.trained_model = None # é‡ç½®æ¨¡å‹
            st.session_state.predictions = None # é‡ç½®é¢„æµ‹ç»“æœ
            return True
        else:
            st.error(f"æ•°æ®å¤„ç†å™¨æœªèƒ½ä» {file_path} åŠ è½½æ•°æ®ã€‚")
            return False
    except Exception as e:
        st.error(f"åŠ è½½æ•°æ® '{os.path.basename(file_path)}' æ—¶å‡ºé”™: {e}")
        st.session_state.raw_df = None
        return False

def load_sample_data():
    """ä»æœ¬åœ°ç¼“å­˜åŠ è½½åœ¨çº¿é›¶å”®æ•°æ®é›†ï¼Œå¦‚æœæœ¬åœ°æ²¡æœ‰åˆ™å°è¯•ä»UCIè·å–ã€‚"""
    sample_data_path = os.path.join(RAW_DATA_DIR, DEFAULT_DATA_FILE)
    if os.path.exists(sample_data_path):
        st.info(f"æ‰¾åˆ°æœ¬åœ°æ•°æ®æ–‡ä»¶: '{sample_data_path}'ã€‚æ­£åœ¨åŠ è½½...")
        load_data_from_path(sample_data_path)
    else:
        st.info(f"'{DEFAULT_DATA_FILE}' åœ¨ '{RAW_DATA_DIR}' ä¸­æœªæ‰¾åˆ°ã€‚æ­£åœ¨å°è¯•ä» UCI ä»£ç åº“è·å–...")
        try:
            from ucimlrepo import fetch_ucirepo
            online_retail_uci = fetch_ucirepo(id=352) # Online Retail UCI ID
            df_X = online_retail_uci.data.features

            if df_X is not None and not df_X.empty:
                try:
                    df_X.to_excel(sample_data_path, index=False) # ä¿å­˜ä¸º Excel
                    st.success(f"æ•°æ®é›†å·²ä» UCI è·å–å¹¶ä¿å­˜åˆ° {sample_data_path}")
                    load_data_from_path(sample_data_path) # ä»æ–°ä¿å­˜çš„æ–‡ä»¶åŠ è½½
                except Exception as e:
                    st.warning(f"æ— æ³•å°†è·å–çš„æ•°æ®ä¿å­˜åˆ° {sample_data_path}: {e}ã€‚æ‚¨å¯èƒ½éœ€è¦æ‰‹åŠ¨ä¸‹è½½ã€‚")
                    st.info("å°†å°è¯•ä½¿ç”¨å†…å­˜ä¸­çš„æ•°æ®...")
                    st.session_state.raw_df = df_X # ä½¿ç”¨å†…å­˜ä¸­çš„æ•°æ®
                    # æ³¨æ„: DataProcessor é€šå¸¸éœ€è¦ä¸€ä¸ªè·¯å¾„ã€‚å¦‚æœä¿å­˜å¤±è´¥ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½å—é™ã€‚
                    # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„ DataProcessor å®ä¾‹ç”¨äºåŸºæœ¬æ“ä½œï¼Œå¦‚æœéœ€è¦çš„è¯ã€‚
                    # æˆ–è€…æç¤ºç”¨æˆ·ä¸€äº›ä¾èµ–äºè·¯å¾„çš„åŠŸèƒ½å¯èƒ½æ— æ³•ä½¿ç”¨ã€‚
                    st.session_state.data_processor = None # è¡¨ç¤ºDataProcessoræœªä»è·¯å¾„æ­£ç¡®åˆå§‹åŒ–
                    st.success("å·²åŠ è½½å†…å­˜ä¸­çš„ç¤ºä¾‹æ•°æ®ã€‚")

            else:
                st.error("ä» UCI ä»£ç åº“è·å–æ•°æ®ç‰¹å¾å¤±è´¥æˆ–æ•°æ®ä¸ºç©ºã€‚")
                st.session_state.raw_df = None
        except ImportError:
            st.error("æœªæ‰¾åˆ° ucimlrepo åŒ…ã€‚è¯·å®‰è£…: pip install ucimlrepo")
        except Exception as e:
            st.error(f"ä» UCI è·å–æ•°æ®é›†æ—¶å‡ºé”™: {e}ã€‚è¯·å°è¯•æ‰‹åŠ¨ä¸‹è½½ã€‚")
            st.markdown(f"æ‚¨å¯ä»¥ä» [UCI Online Retail Dataset](https://archive.ics.uci.edu/dataset/352/online+retail) ä¸‹è½½å®ƒï¼Œå¹¶å°† '{DEFAULT_DATA_FILE}' æ–‡ä»¶ï¼ˆé€šå¸¸æ˜¯ .xlsx æˆ– .csvï¼‰æ”¾å…¥ '{RAW_DATA_DIR}' ç›®å½•ä¸­ã€‚")
            st.session_state.raw_df = None

# --- UI éƒ¨åˆ† ---
st.set_page_config(layout="wide", page_title="ç”µå•†æ™ºæ…§åº“å­˜ç³»ç»Ÿ")
st.title("ğŸ›’ ç”µå•†æ™ºæ…§åº“å­˜ç³»ç»Ÿ")

# --- ä¾§è¾¹æ å¯¼èˆª/æ§åˆ¶ ---
st.sidebar.header("æ§åˆ¶é¢æ¿")
app_mode = st.sidebar.selectbox(
    "é€‰æ‹©åŠŸèƒ½æ¨¡å—",
    ["æ•°æ®æ¦‚è§ˆä¸å¤„ç†", "éœ€æ±‚é¢„æµ‹å»ºæ¨¡", "åº“å­˜å¯è§†åŒ–", "å…³äº"]
)

if st.sidebar.button("åŠ è½½åœ¨çº¿é›¶å”®æ•°æ®é›† (ç¤ºä¾‹)"):
    load_sample_data()

# --- åŸºäºæ¨¡å¼çš„ä¸»é¡µé¢å†…å®¹ ---
if app_mode == "æ•°æ®æ¦‚è§ˆä¸å¤„ç†":
    st.header("ğŸ“Š æ•°æ®æ¦‚è§ˆä¸å¤„ç†")
    
    uploaded_file = st.file_uploader("ä¸Šä¼ æ‚¨çš„é”€å”®æ•°æ® (CSV æˆ– XLSX)", type=["csv", "xlsx"])
    if uploaded_file is not None:
        file_path = os.path.join(RAW_DATA_DIR, uploaded_file.name)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        st.info(f"æ–‡ä»¶ '{uploaded_file.name}' å·²ä¿å­˜åˆ° {file_path}")
        load_data_from_path(file_path)

    if st.session_state.get('raw_df') is not None:
        st.subheader("åŸå§‹æ•°æ®é¢„è§ˆ")
        st.dataframe(st.session_state.raw_df.head())
        st.write(f"åŸå§‹æ•°æ®å½¢çŠ¶: {st.session_state.raw_df.shape}")

        if st.session_state.data_processor:
            st.markdown("#### æ•°æ®æ¸…æ´—é€‰é¡¹")
            col1, col2 = st.columns(2)
            with col1:
                remove_cancelled = st.checkbox("ç§»é™¤å·²å–æ¶ˆ/è´Ÿæ•°æ•°é‡çš„è®¢å•?", value=True, key='cb_remove_cancelled')
                handle_outliers_qty = st.checkbox("å¤„ç†'æ•°é‡(Quantity)'åˆ—çš„å¼‚å¸¸å€¼(IQR)?", value=False, key='cb_handle_outliers_qty')
            with col2:
                remove_zero_price = st.checkbox("ç§»é™¤å•ä»·(UnitPrice)ä¸º0æˆ–è´Ÿæ•°çš„å•†å“?", value=True, key='cb_remove_zero_price')
                handle_outliers_price = st.checkbox("å¤„ç†'å•ä»·(UnitPrice)'åˆ—çš„å¼‚å¸¸å€¼(IQR)?", value=False, key='cb_handle_outliers_price')

            if st.button("æ‰§è¡Œæ•°æ®æ¸…æ´—ä¸è½¬æ¢", key="btn_process_data"):
                with st.spinner("æ­£åœ¨æ¸…æ´—ä¸è½¬æ¢æ•°æ®..."):
                    try:
                        cleaned_df = st.session_state.data_processor.clean_data(
                            st.session_state.raw_df.copy(), # ä½¿ç”¨åŸå§‹æ•°æ®çš„å‰¯æœ¬
                            remove_cancelled_orders=remove_cancelled,
                            remove_zero_unit_price=remove_zero_price,
                            handle_outliers_quantity=handle_outliers_qty,
                            handle_outliers_unitprice=handle_outliers_price
                        )
                        
                        if cleaned_df is not None and not cleaned_df.empty:
                            transformed_df = st.session_state.data_processor.transform_data(cleaned_df.copy())
                            st.session_state.processed_df = transformed_df
                            st.success("æ•°æ®æ¸…æ´—ä¸è½¬æ¢å®Œæˆ!")
                            
                            if st.session_state.processed_df is not None and not st.session_state.processed_df.empty:
                                original_filename_stem = Path(st.session_state.data_processor.data_path).stem
                                processed_file_path_parquet = os.path.join(PROCESSED_DATA_DIR, f"{original_filename_stem}_processed.parquet")
                                st.session_state.data_processor.processed_data = st.session_state.processed_df 
                                st.session_state.data_processor.save_processed_data(processed_file_path_parquet)
                                st.info(f"å¤„ç†åçš„æ•°æ®å·²ä¿å­˜åˆ°: {processed_file_path_parquet} (Parquetæ ¼å¼)")
                            else:
                                st.warning("æ•°æ®è½¬æ¢åç»“æœä¸ºç©ºæˆ–æ— æ•ˆï¼Œæœªä¿å­˜ã€‚")
                                st.session_state.processed_df = None 
                        else:
                            st.warning("æ•°æ®æ¸…æ´—åä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåç»­è½¬æ¢ã€‚è¯·æ£€æŸ¥æ¸…æ´—è§„åˆ™æˆ–åŸå§‹æ•°æ®ã€‚")
                            st.session_state.processed_df = None

                    except Exception as e:
                        st.error(f"æ•°æ®å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                        st.exception(e) 
                        st.session_state.processed_df = None 
            
            if st.session_state.get('processed_df') is not None:
                if not st.session_state.processed_df.empty:
                    st.subheader("å¤„ç†åæ•°æ®é¢„è§ˆ")
                    st.dataframe(st.session_state.processed_df.head())
                    st.write(f"å¤„ç†åæ•°æ®å½¢çŠ¶: {st.session_state.processed_df.shape}")
                else:
                    st.info("æ•°æ®å·²å¤„ç†ï¼Œä½†ç»“æœæ•°æ®é›†ä¸ºç©ºã€‚")
        else:
            st.warning("æ•°æ®å¤„ç†å™¨æœªåˆå§‹åŒ–ã€‚è¯·å…ˆåŠ è½½æ•°æ®ã€‚")
    else:
        st.info("è¯·å…ˆåŠ è½½æˆ–ä¸Šä¼ æ•°æ®ä»¥è¿›è¡Œå¤„ç†ã€‚")

elif app_mode == "éœ€æ±‚é¢„æµ‹å»ºæ¨¡":
    st.header("ğŸ“ˆ éœ€æ±‚é¢„æµ‹å»ºæ¨¡")
    if st.session_state.get('processed_df') is not None and not st.session_state.processed_df.empty:
        processed_df = st.session_state.processed_df
        
        st.subheader("é€‰æ‹©å»ºæ¨¡æ–¹æ³•å’Œå‚æ•°")
        model_type = st.selectbox("é€‰æ‹©æ¨¡å‹ç±»å‹", ["RandomForest", "SARIMA"], key="model_type_select")
        
        # è·å–æ•°æ®ä¸­çš„æ—¥æœŸåˆ—å’Œæ•°å€¼åˆ—ç”¨äºæ¨¡å‹é€‰æ‹©
        datetime_cols = processed_df.select_dtypes(include=['datetime64[ns]']).columns.tolist()
        numeric_cols = processed_df.select_dtypes(include=np.number).columns.tolist()

        if not datetime_cols:
            st.warning("å¤„ç†åçš„æ•°æ®ä¸­æœªæ‰¾åˆ°æ—¥æœŸæ—¶é—´åˆ—ã€‚SARIMAæ¨¡å‹å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œã€‚")
        
        target_column = st.selectbox("é€‰æ‹©ç›®æ ‡å˜é‡ (éœ€è¦é¢„æµ‹çš„åˆ—ï¼Œé€šå¸¸æ˜¯é”€é‡æˆ–æ•°é‡)", numeric_cols, key="target_col_model")

        # --- RandomForest ç‰¹å®šå‚æ•° ---
        if model_type == 'RandomForest':
            st.markdown("#### RandomForest å‚æ•°")
            # è‡ªåŠ¨æ’é™¤æ—¥æœŸåˆ—å’Œç›®æ ‡åˆ—ä½œä¸ºé»˜è®¤ç‰¹å¾å€™é€‰
            potential_feature_cols_rf = [col for col in numeric_cols + processed_df.select_dtypes(include=['object','category']).columns.tolist() 
                                         if col != target_column and col not in datetime_cols]
            
            # å°è¯•è‡ªåŠ¨é€‰æ‹©ä¸€äº›å¸¸è§çš„æ—¥æœŸè¡ç”Ÿç‰¹å¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            date_derived_features = ['OrderYear', 'OrderMonth', 'OrderDay', 'OrderDayOfWeek', 'OrderHour', 'WeekOfYear', 'OrderQuarter']
            default_features_rf = [col for col in date_derived_features if col in processed_df.columns and col in potential_feature_cols_rf]
            # å¦‚æœæ²¡æœ‰æ—¥æœŸè¡ç”Ÿç‰¹å¾ï¼Œå¯ä»¥è€ƒè™‘å…¶ä»–æ•°å€¼æˆ–åˆ†ç±»ç‰¹å¾
            if not default_features_rf and potential_feature_cols_rf:
                 default_features_rf = potential_feature_cols_rf[:min(3, len(potential_feature_cols_rf))] # æœ€å¤šé€‰3ä¸ª

            feature_columns_rf = st.multiselect("é€‰æ‹©ç‰¹å¾åˆ— (Features)", potential_feature_cols_rf, default=default_features_rf, key="rf_features")
            
            col_rf1, col_rf2 = st.columns(2)
            with col_rf1:
                n_estimators_rf = st.number_input("å†³ç­–æ ‘æ•°é‡ (n_estimators)", min_value=10, max_value=500, value=100, step=10, key="rf_n_est")
                max_depth_rf = st.number_input("æœ€å¤§æ·±åº¦ (max_depth)", min_value=1, max_value=50, value=10, step=1, key="rf_max_depth")
            with col_rf2:
                random_state_rf = st.number_input("éšæœºç§å­ (random_state)", value=42, step=1, key="rf_rand_state")
                test_size_rf = st.slider("æµ‹è¯•é›†æ¯”ä¾‹ (test_size)", min_value=0.1, max_value=0.5, value=0.2, step=0.05, key="rf_test_size")

        # --- SARIMA ç‰¹å®šå‚æ•° ---
        elif model_type == "SARIMA":
            if not datetime_cols:
                st.error("SARIMA æ¨¡å‹éœ€è¦ä¸€ä¸ªæ—¥æœŸæ—¶é—´ç±»å‹çš„ç´¢å¼•æˆ–åˆ—ã€‚è¯·ç¡®ä¿æ•°æ®å·²æ­£ç¡®å¤„ç†ã€‚")
            else:
                # SARIMA é€šå¸¸ä½œç”¨äºä¸€ä¸ªä»¥æ—¥æœŸæ—¶é—´ä¸ºç´¢å¼•çš„å•å˜é‡åºåˆ—
                # æç¤ºç”¨æˆ·é€‰æ‹©æ—¥æœŸåˆ—ä½œä¸ºç´¢å¼•ï¼ˆå¦‚æœæ•°æ®ä¸­å·²æœ‰å¤šä¸ªæ—¥æœŸåˆ—ï¼‰
                date_col_for_index = None
                if 'InvoiceDate' in datetime_cols: # ä¼˜å…ˆä½¿ç”¨åŸå§‹å‘ç¥¨æ—¥æœŸ
                    date_col_for_index = 'InvoiceDate'
                elif datetime_cols: # å¦åˆ™é€‰æ‹©ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„æ—¥æœŸåˆ—
                    date_col_for_index = datetime_cols[0]

                if date_col_for_index:
                    st.info(f"SARIMA æ¨¡å‹å°†å°è¯•ä½¿ç”¨ '{date_col_for_index}' åˆ—ä½œä¸ºæ—¶é—´ç´¢å¼•ï¼Œå¹¶å¯¹ '{target_column}' è¿›è¡Œé¢„æµ‹ã€‚")
                    st.write("è¯·ç¡®ä¿æ‚¨çš„ç›®æ ‡åˆ—æ˜¯æŒ‰æ­¤æ—¥æœŸèšåˆçš„æ—¶é—´åºåˆ—æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œæ¯æ—¥/æ¯å‘¨/æ¯æœˆæ€»é”€å”®é¢ï¼‰ã€‚")
                else:
                    st.warning("æ— æ³•è‡ªåŠ¨ç¡®å®šç”¨äºSARIMAçš„æ—¥æœŸåˆ—ã€‚")

                # SARIMA å‚æ•° (p,d,q) å’Œ (P,D,Q,s)
                col_sarima1, col_sarima2, col_sarima3 = st.columns(3)
                with col_sarima1:
                    p = st.number_input("é˜¶æ•° p (AR)", min_value=0, value=1, key="sarima_p")
                    d = st.number_input("å·®åˆ†é˜¶æ•° d (I)", min_value=0, value=1, key="sarima_d")
                    q = st.number_input("é˜¶æ•° q (MA)", min_value=0, value=1, key="sarima_q")
                with col_sarima2:
                    P = st.number_input("å­£èŠ‚æ€§é˜¶æ•° P", min_value=0, value=1, key="sarima_P")
                    D = st.number_input("å­£èŠ‚æ€§å·®åˆ†é˜¶æ•° D", min_value=0, value=0, key="sarima_D")
                    Q = st.number_input("å­£èŠ‚æ€§é˜¶æ•° Q", min_value=0, value=1, key="sarima_Q")
                with col_sarima3:
                    s = st.number_input("å­£èŠ‚æ€§å‘¨æœŸ s (ä¾‹å¦‚ï¼Œæœˆåº¦æ•°æ®ä¸º12ï¼Œå­£åº¦æ•°æ®ä¸º4)", min_value=1, value=12, key="sarima_s")
                
                # å¤–ç”Ÿå˜é‡é€‰æ‹© (å¯é€‰)
                potential_exog_cols_sarima = [col for col in numeric_cols if col != target_column]
                exog_cols_sarima = st.multiselect("é€‰æ‹©å¤–ç”Ÿå˜é‡ (Exogenous Variables) (å¯é€‰)", potential_exog_cols_sarima, key="sarima_exog")
                
                test_size_sarima_steps = st.number_input("ç”¨äºæµ‹è¯•çš„æœŸæ•° (ä¾‹å¦‚ï¼Œæœ€å12æœŸ)", min_value=1, value=12, step=1, key="sarima_test_steps")

        if st.button(f"è®­ç»ƒ {model_type} æ¨¡å‹", key=f"btn_train_{model_type}"):
            if not target_column:
                st.error("è¯·é€‰æ‹©ä¸€ä¸ªç›®æ ‡å˜é‡!")
            else:
                with st.spinner(f"æ­£åœ¨å‡†å¤‡æ•°æ®å¹¶è®­ç»ƒ {model_type} æ¨¡å‹..."):
                    try:
                        model_name = f"{model_type}_{target_column}_{pd.Timestamp.now().strftime('%Y%m%d%H%M%S')}"
                        forecaster = None
                        y_test_actual = None
                        predictions = None
                        
                        if model_type == "RandomForest":
                            if not feature_columns_rf:
                                st.error("RandomForest: è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªç‰¹å¾åˆ—ã€‚")
                            else:
                                forecaster = SalesForecaster(model_type='RandomForest', model_params={
                                    'n_estimators': n_estimators_rf, 
                                    'max_depth': max_depth_rf, 
                                    'random_state': random_state_rf
                                })
                                X_rf, y_rf = forecaster.prepare_features(processed_df, target_column=target_column, feature_columns=feature_columns_rf)
                                X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X_rf, y_rf, test_size=test_size_rf, random_state=random_state_rf, shuffle=False)
                                forecaster.train(y_train_rf, X_train_rf)
                                predictions = forecaster.predict(X_test_rf)
                                y_test_actual = y_test_rf

                        elif model_type == "SARIMA":
                            if not date_col_for_index:
                                st.error("SARIMA: æ— æ³•ç¡®å®šæ—¥æœŸåˆ—ï¼Œæ— æ³•ç»§ç»­ã€‚")
                            else:
                                sarima_df_indexed = processed_df.set_index(date_col_for_index) # åŸå§‹æ•°æ®è®¾ç½®ç´¢å¼•
                                
                                resample_freq_sarima = st.text_input("ç›®æ ‡æ—¶é—´åºåˆ—é¢‘ç‡ (ä¾‹å¦‚ 'D', 'W', 'MS')", value='MS', key="sarima_data_freq_input")
                                aggregation_method_target = st.selectbox(f"ç›®æ ‡åˆ— '{target_column}' çš„èšåˆæ–¹æ³•", ['sum', 'mean', 'median', 'first', 'last'], index=0, key="sarima_agg_target_method")

                                if not resample_freq_sarima:
                                    st.error("è¯·è¾“å…¥æœ‰æ•ˆçš„æ—¶é—´åºåˆ—é¢‘ç‡ã€‚")
                                else:
                                    try:
                                        # 1. èšåˆç›®æ ‡å˜é‡åˆ°æŒ‡å®šé¢‘ç‡ï¼Œç¡®ä¿ç´¢å¼•å”¯ä¸€
                                        y_sarima_resampled = sarima_df_indexed[target_column].resample(resample_freq_sarima).agg(aggregation_method_target)
                                        y_sarima = y_sarima_resampled.fillna(method='ffill') # å¡«å……NAå€¼
                                        
                                        st.write(f"èšåˆåŠå¡«å……åçš„ç›®æ ‡åºåˆ— (y_sarima) ç´¢å¼•æ˜¯å¦æœ‰é‡å¤: {y_sarima.index.has_duplicates}")
                                        st.dataframe(y_sarima.head())

                                        if y_sarima.index.has_duplicates:
                                            st.error("é”™è¯¯ï¼šèšåˆåçš„ç›®æ ‡åºåˆ—ç´¢å¼•ä»ç„¶å­˜åœ¨é‡å¤å€¼ã€‚è¯·æ£€æŸ¥æ•°æ®å’Œèšåˆé€»è¾‘ã€‚")
                                            st.stop() # åœæ­¢æ‰§è¡Œï¼Œå› ä¸ºåç»­ä¼šå‡ºé”™

                                        # è°ƒæ•´åçš„æ•°æ®ç‚¹æ£€æŸ¥æ¡ä»¶
                                        min_points_needed = max(s, d if d else 0, D * s if D and s else 0) + test_size_sarima_steps + 1 # ç¡®ä¿då’ŒD*sæœ‰é»˜è®¤å€¼
                                        if len(y_sarima) < min_points_needed:
                                             st.warning(f"è­¦å‘Š: èšåˆåæ•°æ®ç‚¹ ({len(y_sarima)}) è¾ƒå°‘ã€‚æ¨¡å‹å¯èƒ½æ— æ³•ç¨³å®šè®­ç»ƒæˆ–ç»“æœä¸å¯é ã€‚å»ºè®®è‡³å°‘éœ€è¦ {min_points_needed} ç‚¹ã€‚")
                                             # ä¸å†ç›´æ¥ st.error å’Œ st.stop()ï¼Œè€Œæ˜¯å…è®¸ç»§ç»­ä½†ç»™å‡ºè­¦å‘Š
                                        # else:
                                        #     st.success(f"èšåˆåæ•°æ®ç‚¹ ({len(y_sarima)}) æ»¡è¶³æœ€ä½è¦æ±‚ ({min_points_needed} ç‚¹)ã€‚")

                                        # å³ä½¿æ•°æ®ç‚¹è¾ƒå°‘ï¼Œä¹Ÿå°è¯•ç»§ç»­è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•é›†çš„åˆ’åˆ†
                                        if len(y_sarima) <= test_size_sarima_steps:
                                            st.error(f"èšåˆåæ•°æ®ç‚¹ ({len(y_sarima)}) ä¸è¶³ä»¥è¦†ç›–æµ‹è¯•æœŸæ•° ({test_size_sarima_steps})ã€‚æ— æ³•ç»§ç»­ã€‚")
                                            st.stop()
                                        else:
                                            y_train_sarima = y_sarima[:-test_size_sarima_steps]
                                            y_test_actual = y_sarima[-test_size_sarima_steps:]
                                            
                                            X_train_sarima_exog = None
                                            X_test_sarima_exog = None

                                            if exog_cols_sarima:
                                                X_sarima_exog_list = []
                                                temp_exog_df = sarima_df_indexed[exog_cols_sarima] # ä»åŸå§‹å¸¦ç´¢å¼•çš„DataFrameå–å¤–ç”Ÿå˜é‡

                                                for ex_col in exog_cols_sarima:
                                                    aggregation_method_exog = st.selectbox(f"å¤–ç”Ÿå˜é‡ '{ex_col}' çš„èšåˆæ–¹æ³•", ['mean', 'sum', 'median', 'first', 'last'], index=0, key=f"sarima_exog_agg_{ex_col}_method")
                                                    exog_col_resampled = temp_exog_df[ex_col].resample(resample_freq_sarima).agg(aggregation_method_exog)
                                                    exog_col_filled = exog_col_resampled.fillna(method='ffill')
                                                    X_sarima_exog_list.append(exog_col_filled.rename(ex_col))
                                                
                                                if X_sarima_exog_list:
                                                    X_sarima_exog_df_full = pd.concat(X_sarima_exog_list, axis=1)
                                                    st.write(f"èšåˆåŠå¡«å……åçš„å¤–ç”Ÿå˜é‡ (X_sarima_exog_df_full) ç´¢å¼•æ˜¯å¦æœ‰é‡å¤: {X_sarima_exog_df_full.index.has_duplicates}")
                                                    st.dataframe(X_sarima_exog_df_full.head())

                                                    if X_sarima_exog_df_full.index.has_duplicates:
                                                        st.error("é”™è¯¯ï¼šèšåˆåçš„å¤–ç”Ÿå˜é‡ç´¢å¼•å­˜åœ¨é‡å¤å€¼ã€‚")
                                                        st.stop()

                                                    # å¯¹é½ y_sarima å’Œ X_sarima_exog_df_full çš„ç´¢å¼•
                                                    common_idx = y_sarima.index.intersection(X_sarima_exog_df_full.index)
                                                    y_sarima_aligned = y_sarima.loc[common_idx]
                                                    X_sarima_exog_df_aligned = X_sarima_exog_df_full.loc[common_idx]

                                                    # è°ƒæ•´åçš„ä¸å¤–ç”Ÿå˜é‡å¯¹é½åçš„æ•°æ®ç‚¹æ£€æŸ¥
                                                    min_points_needed_aligned = max(s, d if d else 0, D * s if D and s else 0) + test_size_sarima_steps + 1
                                                    if len(y_sarima_aligned) < min_points_needed_aligned:
                                                        st.warning(f"è­¦å‘Š: ä¸å¤–ç”Ÿå˜é‡å¯¹é½ç´¢å¼•åï¼Œæ•°æ®ç‚¹ ({len(y_sarima_aligned)}) è¾ƒå°‘ã€‚æ¨¡å‹å¯èƒ½æ— æ³•ç¨³å®šè®­ç»ƒæˆ–ç»“æœä¸å¯é ã€‚å»ºè®®è‡³å°‘éœ€è¦ {min_points_needed_aligned} ç‚¹ã€‚")
                                                    # else:
                                                    #    st.success(f"ä¸å¤–ç”Ÿå˜é‡å¯¹é½åæ•°æ®ç‚¹ ({len(y_sarima_aligned)}) æ»¡è¶³æœ€ä½è¦æ±‚ ({min_points_needed_aligned} ç‚¹)ã€‚")
                                                    
                                                    if len(y_sarima_aligned) <= test_size_sarima_steps:
                                                        st.error(f"ä¸å¤–ç”Ÿå˜é‡å¯¹é½ç´¢å¼•åï¼Œæ•°æ®ç‚¹ ({len(y_sarima_aligned)}) ä¸è¶³ä»¥è¦†ç›–æµ‹è¯•æœŸæ•° ({test_size_sarima_steps})ã€‚æ— æ³•ç»§ç»­ã€‚")
                                                        st.stop()
                                                    else:
                                                        y_train_sarima = y_sarima_aligned[:-test_size_sarima_steps]
                                                        y_test_actual = y_sarima_aligned[-test_size_sarima_steps:]
                                                        X_train_sarima_exog = X_sarima_exog_df_aligned.loc[y_train_sarima.index]
                                                        X_test_sarima_exog = X_sarima_exog_df_aligned.loc[y_test_actual.index]
                                            
                                            # ç¡®ä¿ y_train_sarima ä¼ é€’ç»™ forecaster
                                            st.write("ä¼ é€’ç»™æ¨¡å‹è®­ç»ƒçš„ y_train_sarima ç´¢å¼•æ˜¯å¦æœ‰é‡å¤:", y_train_sarima.index.has_duplicates)
                                            if X_train_sarima_exog is not None:
                                                st.write("ä¼ é€’ç»™æ¨¡å‹è®­ç»ƒçš„ X_train_sarima_exog ç´¢å¼•æ˜¯å¦æœ‰é‡å¤:", X_train_sarima_exog.index.has_duplicates)

                                            forecaster = SalesForecaster(model_type='SARIMA', 
                                                                 order=(p,d,q), 
                                                                 seasonal_order=(P,D,Q,s), 
                                                                 exog_cols=exog_cols_sarima if exog_cols_sarima else None)
                                            forecaster.train(y_train_sarima, X_train_sarima_exog) # y_train_sarima åº”è¯¥æ˜¯èšåˆåçš„
                                            predictions = forecaster.predict(steps=len(y_test_actual), X_test=X_test_sarima_exog)
                                            if isinstance(predictions, pd.Series) and isinstance(y_test_actual, pd.Series):
                                                predictions.index = y_test_actual.index 
                                    except Exception as data_prep_ex:
                                        st.error(f"SARIMAæ•°æ®å‡†å¤‡/èšåˆæ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {data_prep_ex}")
                                        st.exception(data_prep_ex)

                        if predictions is not None:
                            st.session_state.trained_model = {
                                'forecaster': forecaster,
                                'y_test': y_test_actual,
                                'predictions': predictions,
                                'model_type': model_type,
                                'target_column': target_column 
                            }
                            st.session_state.predictions = predictions
                            st.session_state.last_trained_model_name = model_name # Store model name
                            st.success(f"{model_type} æ¨¡å‹è®­ç»ƒå®Œæˆ!")
                        else:
                            st.error(f"{model_type} æ¨¡å‹è®­ç»ƒå¤±è´¥ã€‚")
                            st.session_state.last_trained_model_name = None # Reset if failed
                    except Exception as e:
                        st.error(f"æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿä¸»è¦é”™è¯¯: {e}")
                        st.exception(e)
                        st.session_state.last_trained_model_name = None # Reset on error

        if st.session_state.get('predictions') is not None:
            st.subheader("é¢„æµ‹ç»“æœ")
            # Ensure predictions is a DataFrame or Series that st.dataframe can handle
            if isinstance(st.session_state.predictions, (pd.Series, pd.DataFrame, np.ndarray)):
                st.dataframe(st.session_state.predictions)
            if hasattr(st.session_state.predictions, 'shape'):
                st.write(f"é¢„æµ‹ç»“æœå½¢çŠ¶: {st.session_state.predictions.shape}")
            else:
                st.write("é¢„æµ‹ç»“æœçš„æ ¼å¼æ— æ³•ç›´æ¥æ˜¾ç¤ºã€‚")


            _forecaster_for_display_and_save = None
            if st.session_state.get('trained_model') and st.session_state.trained_model.get('forecaster'):
                _forecaster_for_display_and_save = st.session_state.trained_model['forecaster']
            
            if _forecaster_for_display_and_save:
                if _forecaster_for_display_and_save.model_type == 'RandomForest':
                    st.subheader("ç‰¹å¾é‡è¦æ€§")
                    feature_importances_df = _forecaster_for_display_and_save.get_feature_importances()
                    if feature_importances_df is not None and not feature_importances_df.empty:
                        # Ensure 'Feature' column exists before setting it as index
                        if 'Feature' in feature_importances_df.columns:
                             st.bar_chart(feature_importances_df.set_index('Feature'))
                        else:
                             st.warning("ç‰¹å¾é‡è¦æ€§æ•°æ®æ¡†ä¸­ç¼ºå°‘ 'Feature' åˆ—ã€‚")
                    else:
                        st.info("æœªèƒ½è·å– RandomForest æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§ã€‚")
                elif _forecaster_for_display_and_save.model_type == 'SARIMA':
                    st.subheader("æ¨¡å‹è¯Šæ–­")
                    _forecaster_for_display_and_save.plot_diagnostics(st) # Pass st object

            if st.button("ä¿å­˜æ¨¡å‹"):
                if st.session_state.get('trained_model') and st.session_state.trained_model.get('forecaster') and st.session_state.get('last_trained_model_name'):
                    _forecaster_to_save = st.session_state.trained_model['forecaster']
                    _model_name_to_save = st.session_state.last_trained_model_name
                    
                    _file_extension = ".joblib" if _forecaster_to_save.model_type == "RandomForest" else ".pkl"
                    _actual_model_save_path = os.path.join(MODEL_DIR, f"{_model_name_to_save}{_file_extension}")

                    _forecaster_to_save.save_model(_actual_model_save_path)
                    st.success(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {_actual_model_save_path}")
                    st.session_state.last_saved_model_path = _actual_model_save_path
                else:
                    st.warning("æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯ï¼ˆæ¨¡å‹å®ä¾‹ã€forecaster æˆ–æ¨¡å‹åç§°ï¼‰æ¥ä¿å­˜æ¨¡å‹ã€‚è¯·å…ˆæˆåŠŸè®­ç»ƒä¸€ä¸ªæ¨¡å‹ã€‚")
            
            _default_path_for_load_input = st.session_state.get('last_saved_model_path', "")
            if not _default_path_for_load_input and st.session_state.get('last_trained_model_name'):
                _temp_model_info = st.session_state.get('trained_model', {})
                _temp_model_type = _temp_model_info.get('model_type', "RandomForest") # Default if no info
                _temp_ext = ".joblib" if _temp_model_type == "RandomForest" else ".pkl"
                _default_path_for_load_input = os.path.join(MODEL_DIR, f"{st.session_state.last_trained_model_name}{_temp_ext}")
            
            if not _default_path_for_load_input:
                try:
                    model_files = [
                        os.path.join(MODEL_DIR, f) for f in os.listdir(MODEL_DIR) 
                        if os.path.isfile(os.path.join(MODEL_DIR, f)) and (f.endswith(".pkl") or f.endswith(".joblib"))
                    ]
                    if model_files:
                        model_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
                        _default_path_for_load_input = model_files[0]
                except Exception:
                    pass # Silently ignore

            model_load_path_from_input = st.text_input("è¾“å…¥æ¨¡å‹æ–‡ä»¶è·¯å¾„", value=_default_path_for_load_input, key="model_load_path_text_input_v3")

            if st.button("åŠ è½½æ¨¡å‹"):
                final_model_load_path = model_load_path_from_input
                if os.path.exists(final_model_load_path):
                    filename_lower = Path(final_model_load_path).name.lower()
                    ui_selected_model_type = st.session_state.get("model_type_select", "RandomForest")
                    
                    determined_model_type_for_load = ui_selected_model_type # Default to UI selection
                    if ".pkl" in filename_lower and "sarima" in filename_lower:
                        determined_model_type_for_load = "SARIMA"
                    elif ".joblib" in filename_lower and ("randomforest" in filename_lower or "rf" in filename_lower):
                        determined_model_type_for_load = "RandomForest"
                    elif ".pkl" in filename_lower: # Could be SARIMA or other statsmodels
                        determined_model_type_for_load = "SARIMA" # Assume SARIMA for .pkl if not clearly RF
                    elif ".joblib" in filename_lower: # Could be RF or other sklearn
                         determined_model_type_for_load = "RandomForest"


                    loaded_forecaster_instance = SalesForecaster(model_type=determined_model_type_for_load)
                    try:
                        loaded_forecaster_instance.load_model(final_model_load_path)
                        
                        st.session_state.trained_model = {
                            'forecaster': loaded_forecaster_instance,
                            'y_test': None, 
                            'predictions': None, 
                            'model_type': loaded_forecaster_instance.model_type,
                            'target_column': getattr(loaded_forecaster_instance, 'target_column', None) # if stored
                        }
                        st.session_state.predictions = None
                        st.session_state.last_trained_model_name = Path(final_model_load_path).stem
                        st.session_state.last_saved_model_path = final_model_load_path

                        st.success(f"æ¨¡å‹ ({loaded_forecaster_instance.model_type}) å·²ä» {final_model_load_path} åŠ è½½ã€‚")
                        st.info("å¦‚éœ€è¯„ä¼°æˆ–æŸ¥çœ‹é¢„æµ‹ï¼Œè¯·ä½¿ç”¨æ–°æ•°æ®æˆ–é‡æ–°è¿è¡Œé¢„æµ‹æ­¥éª¤ã€‚")
                    except Exception as e_load:
                        st.error(f"åŠ è½½æ¨¡å‹ {final_model_load_path} æ—¶å‡ºé”™: {e_load}")
                    st.exception(e_load)
                else:
                    st.error(f"æ–‡ä»¶ {final_model_load_path} ä¸å­˜åœ¨ã€‚")

elif app_mode == "åº“å­˜å¯è§†åŒ–":
    st.header("ğŸ“Š åº“å­˜å¯è§†åŒ–")
    if st.session_state.get('processed_df') is not None and not st.session_state.processed_df.empty:
        processed_df = st.session_state.processed_df
        
        st.subheader("é€‰æ‹©å¯è§†åŒ–ç±»å‹å’Œå‚æ•°")
        plot_type = st.selectbox("é€‰æ‹©å¯è§†åŒ–ç±»å‹", ["æ—¶é—´åºåˆ—åˆ†è§£", "é”€é‡è¶‹åŠ¿", "åº“å­˜æ°´å¹³", "å…¶ä»–"], key="plot_type_select")
        
        if plot_type == "æ—¶é—´åºåˆ—åˆ†è§£":
            datetime_cols = processed_df.select_dtypes(include=['datetime64[ns]']).columns.tolist()
            if datetime_cols:
                date_col_for_decompose = st.selectbox("é€‰æ‹©æ—¥æœŸåˆ—", datetime_cols, key="decompose_date_col")
                
                numeric_cols_for_decompose = processed_df.select_dtypes(include=np.number).columns.tolist()
                if not numeric_cols_for_decompose:
                    st.error("å¤„ç†åçš„æ•°æ®ä¸­æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ•°å€¼åˆ—ä½œä¸ºç›®æ ‡åˆ—è¿›è¡Œæ—¶é—´åºåˆ—åˆ†è§£ã€‚è¯·æ£€æŸ¥æ•°æ®å¤„ç†æ­¥éª¤ã€‚")
                    st.stop()
                target_col_for_decompose = st.selectbox("é€‰æ‹©æ•°å€¼å‹ç›®æ ‡åˆ—è¿›è¡Œåˆ†è§£", numeric_cols_for_decompose, key="decompose_target_col")
                
                period_for_decompose = st.number_input("è¾“å…¥å­£èŠ‚æ€§å‘¨æœŸ (ä¾‹å¦‚ï¼Œæœˆåº¦æ•°æ®å¡«12ï¼Œå‘¨åº¦æ•°æ®å¡«7)", min_value=2, value=12, step=1, key="decompose_period")

                st.markdown("--- Optional: Resampling ---")
                apply_resampling = st.checkbox("å¯¹æ•°æ®è¿›è¡Œé‡é‡‡æ ·?", value=False, key="decompose_apply_resample")
                resample_rule = None
                resample_agg_method = None

                if apply_resampling:
                    resample_rule = st.selectbox(
                        "é€‰æ‹©é‡é‡‡æ ·é¢‘ç‡", 
                        options=['D', 'W', 'MS', 'M', 'QS', 'Q', 'AS', 'A'], 
                        index=2, 
                        key="decompose_resample_rule",
                        help="D: Daily, W: Weekly, MS: Month Start, M: Month End, QS: Quarter Start, Q: Quarter End, AS: Year Start, A: Year End"
                    )
                    resample_agg_method = st.selectbox(
                        "é€‰æ‹©èšåˆæ–¹æ³•", 
                        options=['sum', 'mean', 'median', 'first', 'last', 'count'], 
                        index=0,
                        key="decompose_resample_agg"
                    )
                
                if st.button("ç”Ÿæˆæ—¶é—´åºåˆ—åˆ†è§£å›¾"):
                    if target_col_for_decompose and date_col_for_decompose:
                        if not pd.api.types.is_numeric_dtype(processed_df[target_col_for_decompose]):
                            st.error(f"é€‰æ‹©çš„ç›®æ ‡åˆ— '{target_col_for_decompose}' ä¸æ˜¯æ•°å€¼ç±»å‹ã€‚è¯·é€‰æ‹©ä¸€ä¸ªæ•°å€¼åˆ—æˆ–åœ¨æ•°æ®å¤„ç†é˜¶æ®µè½¬æ¢è¯¥åˆ—ã€‚")
                        else:
                            with st.spinner("æ­£åœ¨ç”Ÿæˆæ—¶é—´åºåˆ—åˆ†è§£å›¾..."):
                                try:
                                    data_to_decompose = processed_df.copy()

                                    if apply_resampling and resample_rule and resample_agg_method:
                                        st.info(f"æ­£åœ¨å°†æ•°æ®æŒ‰ '{date_col_for_decompose}' åˆ—ä»¥é¢‘ç‡ '{resample_rule}' ä½¿ç”¨ '{resample_agg_method}' æ–¹æ³•é‡é‡‡æ ·...")
                                        try:
                                            resample_df_indexed = data_to_decompose.set_index(date_col_for_decompose)
                                            resampled_series = resample_df_indexed[target_col_for_decompose].resample(resample_rule).agg(resample_agg_method)
                                            
                                            data_to_decompose = pd.DataFrame(resampled_series)
                                            data_to_decompose = data_to_decompose.reset_index() 
                                            date_col_for_plotter = data_to_decompose.columns[0] 
                                            target_col_for_plotter = target_col_for_decompose 
                                            st.success("æ•°æ®é‡é‡‡æ ·å®Œæˆã€‚")
                                            st.dataframe(data_to_decompose.head())
                                        except Exception as resample_ex:
                                            st.error(f"æ•°æ®é‡é‡‡æ ·è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {resample_ex}")
                                            st.exception(resample_ex)
                                            st.stop()
                                    else:
                                        date_col_for_plotter = date_col_for_decompose
                                        target_col_for_plotter = target_col_for_decompose

                                    if 'plotter' not in st.session_state:
                                        st.session_state.plotter = Plotter()
                                    
                                    fig_decompose = st.session_state.plotter.plot_time_series_decomposition(
                                        data_to_decompose, 
                                        date_column=date_col_for_plotter, 
                                        value_column=target_col_for_plotter, 
                                        period=period_for_decompose
                                    )
                                    if fig_decompose:
                                        st.pyplot(fig_decompose)
                                    else:
                                        st.warning("æœªèƒ½ç”Ÿæˆæ—¶é—´åºåˆ—åˆ†è§£å›¾ã€‚è¯·æ£€æŸ¥æ•°æ®å’Œå‚æ•°ï¼Œæˆ–æŸ¥çœ‹æ§åˆ¶å°è¾“å‡ºã€‚")
                                except Exception as e:
                                    st.error(f"ç”Ÿæˆæ—¶é—´åºåˆ—åˆ†è§£å›¾æ—¶å‡ºé”™: {e}")
                                    st.exception(e)
            else:
                st.warning("å¤„ç†åçš„æ•°æ®ä¸­æœªæ‰¾åˆ°æ—¥æœŸæ—¶é—´åˆ—ã€‚æ— æ³•ç”Ÿæˆæ—¶é—´åºåˆ—åˆ†è§£å›¾ã€‚")

        elif plot_type == "é”€é‡è¶‹åŠ¿":
            datetime_cols = processed_df.select_dtypes(include=['datetime64[ns]']).columns.tolist()
            numeric_cols_for_trend = processed_df.select_dtypes(include=np.number).columns.tolist()

            if datetime_cols and numeric_cols_for_trend:
                date_col_for_trend = st.selectbox("é€‰æ‹©æ—¥æœŸåˆ—", datetime_cols, key="trend_date_col")
                
                target_col_for_trend = st.selectbox(
                    "é€‰æ‹©è¦è§‚å¯Ÿè¶‹åŠ¿çš„æ•°å€¼ç›®æ ‡åˆ—", 
                    numeric_cols_for_trend, 
                    key="trend_target_col_numeric",
                    help="ä¾‹å¦‚ Quantity, UnitPrice, TotalPrice"
                )
                
                agg_method_trend = st.selectbox(
                    "é€‰æ‹©æŒ‰æ—¥æœŸèšåˆç›®æ ‡åˆ—çš„æ–¹æ³•",
                    options=['sum', 'mean', 'median', 'count'], 
                    index=0, 
                    key="trend_agg_method",
                    help="ä¾‹å¦‚ï¼Œé€‰æ‹© 'sum' å¯ä»¥æŸ¥çœ‹æ¯æ—¥/æ¯å‘¨/æ¯æœˆæ€»é”€é‡/æ€»é”€å”®é¢ã€‚"
                )

                resample_freq_trend = st.text_input(
                    "è¾“å…¥èšåˆçš„æ—¶é—´é¢‘ç‡ (å¯é€‰, ä¾‹å¦‚ 'D', 'W', 'MS')", 
                    value='', 
                    key="trend_resample_freq",
                    help="ç•™ç©ºåˆ™æŒ‰åŸå§‹æ—¥æœŸä¸­çš„æ¯ä¸€å¤©èšåˆã€‚D: Daily, W: Weekly, MS: Month Start"
                ).strip()

                if st.button("ç”Ÿæˆé”€é‡è¶‹åŠ¿å›¾"):
                    if date_col_for_trend and target_col_for_trend and agg_method_trend:
                        with st.spinner("æ­£åœ¨ç”Ÿæˆé”€é‡è¶‹åŠ¿å›¾..."):
                            try:
                                trend_data_df = processed_df.copy()
                                
                                if not pd.api.types.is_datetime64_any_dtype(trend_data_df[date_col_for_trend]):
                                    trend_data_df[date_col_for_trend] = pd.to_datetime(trend_data_df[date_col_for_trend], errors='coerce')
                                    trend_data_df.dropna(subset=[date_col_for_trend], inplace=True)

                                trend_data_indexed = trend_data_df.set_index(date_col_for_trend)
                                
                                if resample_freq_trend: 
                                    st.info(f"æŒ‰é¢‘ç‡ '{resample_freq_trend}' é‡é‡‡æ ·æ•°æ®...")
                                    trend_aggregated_series = trend_data_indexed[target_col_for_trend].resample(resample_freq_trend).agg(agg_method_trend)
                                else: 
                                    st.info(f"æŒ‰ '{date_col_for_trend}' ä¸­çš„å”¯ä¸€æ—¥æœŸèšåˆæ•°æ®...")
                                    trend_aggregated_series = trend_data_indexed.groupby(pd.Grouper(level=date_col_for_trend, freq='D'))[target_col_for_trend].agg(agg_method_trend)
                                    if agg_method_trend in ['sum', 'count']:
                                         trend_aggregated_series = trend_aggregated_series[trend_aggregated_series != 0]
                                    elif agg_method_trend in ['mean', 'median']:
                                         trend_aggregated_series = trend_aggregated_series.dropna()

                                if trend_aggregated_series.empty:
                                    st.warning("èšåˆåçš„æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆè¶‹åŠ¿å›¾ã€‚è¯·æ£€æŸ¥æ‚¨çš„é€‰æ‹©æˆ–æ•°æ®ã€‚")
                                else:
                                    trend_plot_df = pd.DataFrame(trend_aggregated_series).reset_index()
                                    date_col_for_plotter_trend = trend_plot_df.columns[0]
                                    target_col_for_plotter_trend = target_col_for_trend

                                    st.dataframe(trend_plot_df.head()) 

                                    if 'plotter' not in st.session_state:
                                        st.session_state.plotter = Plotter()
                                    
                                    fig_trend = st.session_state.plotter.plot_sales_trend(
                                        trend_plot_df, 
                                        date_column=date_col_for_plotter_trend, 
                                        sales_column=target_col_for_plotter_trend
                                    )
                                    if fig_trend:
                                        st.pyplot(fig_trend)
                                    else:
                                        st.warning("æœªèƒ½ç”Ÿæˆé”€é‡è¶‹åŠ¿å›¾ã€‚è¯·æ£€æŸ¥æ•°æ®å’Œå‚æ•°ã€‚")
                            except Exception as e:
                                st.error(f"ç”Ÿæˆé”€é‡è¶‹åŠ¿å›¾æ—¶å‡ºé”™: {e}")
                                st.exception(e)
                    else:
                        st.warning("è¯·ç¡®ä¿å·²é€‰æ‹©æ—¥æœŸåˆ—ã€æ•°å€¼ç›®æ ‡åˆ—å’Œèšåˆæ–¹æ³•ã€‚")
            elif not datetime_cols:
                st.warning("å¤„ç†åçš„æ•°æ®ä¸­æœªæ‰¾åˆ°æ—¥æœŸæ—¶é—´åˆ—ã€‚æ— æ³•ç”Ÿæˆé”€é‡è¶‹åŠ¿å›¾ã€‚")
            elif not numeric_cols_for_trend:
                st.warning("å¤„ç†åçš„æ•°æ®ä¸­æœªæ‰¾åˆ°æ•°å€¼åˆ—ä½œä¸ºç›®æ ‡ã€‚æ— æ³•ç”Ÿæˆé”€é‡è¶‹åŠ¿å›¾ã€‚")

        elif plot_type == "åº“å­˜æ°´å¹³":
            st.subheader("å½“å‰åº“å­˜æ°´å¹³ä¼°ç®—ä¸å¯è§†åŒ–")
            if st.session_state.get('processed_df') is not None and not st.session_state.processed_df.empty:
                processed_df_stock = st.session_state.processed_df

                if not st.session_state.get('data_processor'):
                    st.warning('DataProcessor æœªåˆå§‹åŒ–ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½å—é™ã€‚è¯·ç¡®ä¿æ•°æ®å·²é€šè¿‡"æ•°æ®æ¦‚è§ˆä¸å¤„ç†"æ­£ç¡®åŠ è½½å’Œå¤„ç†ã€‚')
                    st.stop()
                
                data_processor_instance = st.session_state.data_processor

                st.markdown("#### 1. é€‰æ‹©æ•°æ®åˆ—")
                product_id_cols_stock = [col for col in processed_df_stock.columns if processed_df_stock[col].dtype == 'object' or pd.api.types.is_string_dtype(processed_df_stock[col])]
                default_prod_id_stock = None
                if 'StockCode' in product_id_cols_stock: default_prod_id_stock = 'StockCode'
                elif 'Description' in product_id_cols_stock: default_prod_id_stock = 'Description'
                
                product_id_col_stock = st.selectbox(
                    "é€‰æ‹©äº§å“æ ‡è¯†åˆ—", 
                    options=product_id_cols_stock, 
                    index=product_id_cols_stock.index(default_prod_id_stock) if default_prod_id_stock and default_prod_id_stock in product_id_cols_stock else 0,
                    key="stock_product_id_col",
                    help="é€šå¸¸æ˜¯ StockCode æˆ– Description"
                )
                
                qty_cols_stock = processed_df_stock.select_dtypes(include=np.number).columns.tolist()
                default_qty_col_stock = None
                if 'Quantity' in qty_cols_stock: default_qty_col_stock = 'Quantity'

                quantity_col_stock = st.selectbox(
                    "é€‰æ‹©äº¤æ˜“æ•°é‡åˆ—", 
                    options=qty_cols_stock, 
                    index=qty_cols_stock.index(default_qty_col_stock) if default_qty_col_stock and default_qty_col_stock in qty_cols_stock else 0,
                    key="stock_quantity_col",
                    help="ç”¨äºè®¡ç®—å·²å”®æ•°é‡"
                )

                st.markdown("#### 2. è®¾å®šåˆå§‹åº“å­˜ (ä¼°ç®—ç”¨)")
                assumed_initial_stock = st.number_input(
                    "ä¸ºæ‰€æœ‰äº§å“è®¾å®šä¸€ä¸ªç»Ÿä¸€çš„å‡å®šåˆå§‹åº“å­˜å€¼", 
                    min_value=0, 
                    value=1000,
                    step=50, 
                    key="stock_initial_assumed",
                    help="è¿™å°†ç”¨äºä»æ€»é”€é‡ä¸­å‡å»ï¼Œä»¥ä¼°ç®—å½“å‰åº“å­˜ã€‚è¿™æ˜¯ä¸€ä¸ªç®€åŒ–æ¨¡å‹ã€‚"
                )

                if st.button("è®¡ç®—å¹¶æ˜¾ç¤ºå½“å‰ä¼°ç®—åº“å­˜", key="btn_calc_stock"):
                    if product_id_col_stock and quantity_col_stock:
                        with st.spinner("æ­£åœ¨è®¡ç®—äº§å“æ€»é”€é‡å¹¶ä¼°ç®—åº“å­˜..."):
                            total_sold_df = data_processor_instance.calculate_total_quantity_sold(
                                processed_df_stock, 
                                product_id_col=product_id_col_stock, 
                                quantity_col=quantity_col_stock
                            )

                            if total_sold_df is not None:
                                if total_sold_df.empty:
                                    if not processed_df_stock[(processed_df_stock[quantity_col_stock] > 0)].empty:
                                        st.info(f"å·²å°è¯•è®¡ç®—æ€»é”€é‡ï¼Œä½†æœªèƒ½ä¸ºä»»ä½•æœ‰æ•ˆçš„äº§å“ID ('{product_id_col_stock}') æ‰¾åˆ°æ­£æ•°é”€é‡è®°å½•ï¼Œæˆ–è€…æ‰€æœ‰äº§å“IDéƒ½ä¸ºç©º/æ— æ•ˆã€‚")
                                    else:
                                        st.info(f"åœ¨æ•°æ®ä¸­æ²¡æœ‰æ‰¾åˆ° '{quantity_col_stock}' å¤§äº0çš„è®°å½•ï¼Œå› æ­¤æ— æ³•è®¡ç®—ä»»ä½•é”€é‡ã€‚")
                                
                                all_products = pd.DataFrame({product_id_col_stock: processed_df_stock[product_id_col_stock].unique()})
                                all_products = all_products[all_products[product_id_col_stock].notna() & (all_products[product_id_col_stock] != '')]

                                if not total_sold_df.empty:
                                    current_stock_df = pd.merge(all_products, total_sold_df, on=product_id_col_stock, how='left')
                                else: 
                                    current_stock_df = all_products.copy()
                                    current_stock_df['TotalSold'] = 0 

                                current_stock_df['TotalSold'] = current_stock_df['TotalSold'].fillna(0).astype(int)
                                current_stock_df['EstimatedCurrentStock'] = assumed_initial_stock - current_stock_df['TotalSold']
                                
                                st.session_state.current_stock_df = current_stock_df.sort_values(by='EstimatedCurrentStock', ascending=True)
                                if not current_stock_df.empty :
                                     st.success("å½“å‰ä¼°ç®—åº“å­˜è®¡ç®—å®Œæˆ!")
                                elif total_sold_df.empty: 
                                     st.warning("æ²¡æœ‰æœ‰æ•ˆçš„äº§å“IDå¯ç”¨äºåº“å­˜ä¼°ç®—ã€‚")
                                else: 
                                     st.warning("ä¼°ç®—åº“å­˜ä¸ºç©ºï¼Œè¯·æ£€æŸ¥åŸå§‹æ•°æ®ä¸­çš„äº§å“IDã€‚")
                            else: 
                                st.error("è®¡ç®—æ€»é”€é‡æ—¶å‘ç”Ÿé”™è¯¯ã€‚æ— æ³•ä¼°ç®—åº“å­˜ã€‚")
                                st.session_state.current_stock_df = None
                    else:
                        st.warning("è¯·é€‰æ‹©äº§å“æ ‡è¯†åˆ—å’Œæ•°é‡åˆ—ã€‚")
                
                if 'current_stock_df' in st.session_state and st.session_state.current_stock_df is not None and not st.session_state.current_stock_df.empty:
                    st.markdown("#### 3. åº“å­˜å¯è§†åŒ–ä¸ç­›é€‰")
                    current_stock_display_df = st.session_state.current_stock_df
                    
                    view_option_stock = st.selectbox(
                        "é€‰æ‹©æŸ¥çœ‹æ–¹å¼",
                        options=["æ‰€æœ‰äº§å“", "æœç´¢ç‰¹å®šäº§å“", "åº“å­˜æœ€é«˜çš„äº§å“", "åº“å­˜æœ€ä½çš„äº§å“ (å¯èƒ½ç¼ºè´§)"],
                        key="stock_view_option"
                    )

                    num_products_to_show = 10 

                    if view_option_stock == "æ‰€æœ‰äº§å“":
                        st.dataframe(current_stock_display_df)
                    
                    elif view_option_stock == "æœç´¢ç‰¹å®šäº§å“":
                        search_term_stock = st.text_input(f"è¾“å…¥è¦æœç´¢çš„ {product_id_col_stock}", key="stock_search_term")
                        if search_term_stock:
                            results_df = current_stock_display_df[current_stock_display_df[product_id_col_stock].astype(str).str.contains(search_term_stock, case=False, na=False)]
                            st.dataframe(results_df)
                        else:
                            st.info(f"è¯·è¾“å…¥ '{product_id_col_stock}' è¿›è¡Œæœç´¢ã€‚")

                    elif view_option_stock == "åº“å­˜æœ€é«˜çš„äº§å“":
                        num_products_to_show = st.slider("æ˜¾ç¤ºåº“å­˜æœ€é«˜çš„äº§å“æ•°é‡", 5, 50, 10, key="stock_top_n")
                        st.dataframe(current_stock_display_df.sort_values(by='EstimatedCurrentStock', ascending=False).head(num_products_to_show))
                        
                        if st.session_state.get('plotter'):
                            st.subheader(f"åº“å­˜æœ€é«˜çš„ {num_products_to_show} ç§äº§å“")
                            fig_stock_top = st.session_state.plotter.plot_bar_chart(
                                current_stock_display_df.sort_values(by='EstimatedCurrentStock', ascending=False).head(num_products_to_show),
                                x_column=product_id_col_stock,
                                y_column='EstimatedCurrentStock',
                                title=f"åº“å­˜æœ€é«˜çš„ {num_products_to_show} ç§äº§å“",
                                horizontal=True 
                            )
                            if fig_stock_top: st.pyplot(fig_stock_top)

                    elif view_option_stock == "åº“å­˜æœ€ä½çš„äº§å“ (å¯èƒ½ç¼ºè´§)":
                        num_products_to_show = st.slider("æ˜¾ç¤ºåº“å­˜æœ€ä½çš„äº§å“æ•°é‡", 5, 50, 10, key="stock_bottom_n")
                        lowest_stock_df = current_stock_display_df.sort_values(by='EstimatedCurrentStock', ascending=True)
                        st.dataframe(lowest_stock_df.head(num_products_to_show))

                        if st.session_state.get('plotter'):
                            st.subheader(f"åº“å­˜æœ€ä½çš„ {num_products_to_show} ç§äº§å“")
                            fig_stock_bottom = st.session_state.plotter.plot_bar_chart(
                                lowest_stock_df.head(num_products_to_show).sort_values(by='EstimatedCurrentStock', ascending=False), 
                                x_column=product_id_col_stock,
                                y_column='EstimatedCurrentStock',
                                title=f"åº“å­˜æœ€ä½çš„ {num_products_to_show} ç§äº§å“ (å¯èƒ½ç¼ºè´§)",
                                horizontal=True
                            )
                            if fig_stock_bottom: st.pyplot(fig_stock_bottom)
                elif 'current_stock_df' in st.session_state and st.session_state.current_stock_df is not None and st.session_state.current_stock_df.empty:
                     st.info("ä¼°ç®—åº“å­˜ç»“æœä¸ºç©ºã€‚è¿™å¯èƒ½å‘ç”Ÿåœ¨æ‰€æœ‰äº§å“IDéƒ½æ— æ•ˆï¼Œæˆ–è€…åˆå§‹è®¡ç®—æ­¥éª¤æœªèƒ½ç”Ÿæˆä»»ä½•æ•°æ®ã€‚")
            else: 
                st.info('è¯·å…ˆåœ¨"æ•°æ®æ¦‚è§ˆä¸å¤„ç†"æ¨¡å—åŠ è½½å¹¶å¤„ç†æ•°æ®ï¼Œç„¶åæ‰èƒ½è¿›è¡Œåº“å­˜å¯è§†åŒ–ã€‚')
        elif plot_type == "å…¶ä»–":
            st.write("å…¶ä»–å¯è§†åŒ–é€‰é¡¹å¾…å®ç°ã€‚")
    else:
        st.info('è¯·å…ˆåœ¨"æ•°æ®æ¦‚è§ˆä¸å¤„ç†"æ¨¡å—åŠ è½½å¹¶æˆåŠŸå¤„ç†æ•°æ®ï¼Œç„¶åæ‰èƒ½è®¿é—®æ­¤æ¨¡å—ã€‚')


elif app_mode == "å…³äº":
    st.header("ğŸ“š å…³äº")
    st.write("è¿™æ˜¯ä¸€ä¸ªç”µå•†æ™ºæ…§åº“å­˜ç³»ç»Ÿçš„ç¤ºä¾‹åº”ç”¨ã€‚")
    st.write("å®ƒåŒ…æ‹¬ä»¥ä¸‹åŠŸèƒ½æ¨¡å—:")
    st.markdown("- æ•°æ®æ¦‚è§ˆä¸å¤„ç†")
    st.markdown("- éœ€æ±‚é¢„æµ‹å»ºæ¨¡")
    st.markdown("- åº“å­˜å¯è§†åŒ–")
    st.write("æ‚¨å¯ä»¥åœ¨ä¾§è¾¹æ é€‰æ‹©ä¸åŒçš„åŠŸèƒ½æ¨¡å—ã€‚")